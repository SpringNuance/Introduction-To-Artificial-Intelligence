{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment B\n",
    "\n",
    "## Text generation with OpenAI's GPT-2 model\n",
    "\n",
    "This week you explore the language model that you read about in the first exercise on this course [1]. GPT-2 is a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.\n",
    "\n",
    "Even if other models have been published after GPT-2, this model demonstrates the capabilities of a large transformer-based language model and shows off some interesting, fun, and even scary use-cases of the model. The model has 1.5 billion parameters, trained on a dataset of 8 million web pages, and it has been trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains.\n",
    "\n",
    "We try to explore a broad set of capabilities, including the ability to generate conditional synthetic text samples, where we prime the model with an input (i.e., how the text should start) and have it generate a lengthy continuation. The model adapts to the style and content of the conditioning text. This allows the user to generate (more or less) coherent continuations about a topic of their choosing. This implementation is based on [3] with slight modifications for educational purposes. It also goes without saying, we don't take any responsibilty of the content that the AI generates—don't be offended :) Also note that we use the smaller ~500 Mb trained model here for convenience.\n",
    "\n",
    "You might also be interested in this web page which implements some of these same things in a web UI: https://talktotransformer.com\n",
    "\n",
    "#### References\n",
    "\n",
    "[1] Alex Hern, \"New AI fake text generator may be too dangerous to release, say creators\". The Guardian, February 14, 2019.\n",
    "\n",
    "[2] OpenAI, \"Better language models and their implications\". Accessible at the [OpenAI Blog](https://openai.com/blog/better-language-models/), February 14, 2019 \n",
    "\n",
    "[3] Tae Hwan Jung (Jeff Jung), \"Simple text-generator with OpenAI GPT-2 Pytorch implementation\" [GitHub repository](https://github.com/graykode/gpt-2-Pytorch), which uses code and models from [this repo](https://github.com/huggingface/transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment\n",
    "\n",
    "Clone the repo from https://github.com/graykode/gpt-2-Pytorch and install needed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-2-Pytorch'...\n",
      "remote: Enumerating objects: 130, done.\u001b[K\n",
      "remote: Total 130 (delta 0), reused 0 (delta 0), pack-reused 130\u001b[K\n",
      "Receiving objects: 100% (130/130), 2.39 MiB | 9.67 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n",
      "/course/release/Computer-Assignment-B/gpt-2-Pytorch\n",
      "Collecting regex==2017.4.5\n",
      "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
      "\u001b[K     |████████████████████████████████| 601 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2017.4.5-cp38-cp38-linux_x86_64.whl size=545122 sha256=fdce8238fd7b051aa2dd996514a5627588a1a3edcd109eab4656ed03dadbfef1\n",
      "  Stored in directory: /home/wilkinw1/.cache/pip/wheels/45/6d/d9/1c9b861321c9240122cb967b734a80545c9f465be4fcb16f19\n",
      "Successfully built regex\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2017.4.5\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/graykode/gpt-2-Pytorch\n",
    "%cd gpt-2-Pytorch\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder\n",
    "\n",
    "def text_generator(state_dict, text, nsamples=1, unconditional=False, batch_size=1, \n",
    "                   length=-1, temperature=0.7, top_k=40):\n",
    "\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    seed = random.randint(0, 2147483647)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Model\n",
    "    enc = get_encoder()\n",
    "    config = GPT2Config()\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    model = load_weight(model, state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if length == -1:\n",
    "        length = config.n_ctx // 2\n",
    "    elif length > config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "    print(text)\n",
    "    context_tokens = enc.encode(text)\n",
    "\n",
    "    generated = 0\n",
    "    for _ in range(nsamples // batch_size):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=length,\n",
    "            context=context_tokens  if not unconditional else None,\n",
    "            start_token=enc.encoder['<|endoftext|>'] if unconditional else None,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, device=device\n",
    "        )\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model\n",
    "\n",
    "The pre-trained model is available online and in case you want to use this notebook on your own laptop, you need to download the model (e.g., by `curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin`). However, here on jupyter.cs.aalto.fi we can all share the same model dump in order to save some diskspace and bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('/coursedata/gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Unconditional samples\n",
    "\n",
    "We start of simple, with perhaps the least useful mode of the transformer. If you set the 'unconditional mode' to `True`, the generation will not be conditioned on given text. It just spits out some random text it comes up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/512 [00:00<00:27, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:26<00:00, 19.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "<|endoftext|>A lot of people have asked me if I would like to be a part of the program to help a child develop into a successful adult. My answer is yes. I can help you build a life for yourself and your child, and I will do it by providing safe, safe work that will lead to a happy, satisfying life for you and your child.\n",
      "\n",
      "The best way to support yourself and your child is to support yourselves. The best way to support yourself and your child is to support yourself.\n",
      "\n",
      "I will be making this easy for you.\n",
      "\n",
      "I will be putting together a list of things I will be doing, and I will make you feel comfortable with them.\n",
      "\n",
      "There is no better way to support yourself than to support yourself.\n",
      "\n",
      "I will be giving you my support and yours.\n",
      "\n",
      "I will be making this easy for you.\n",
      "\n",
      "You will be learning to love your little sister, to love you unconditionally, and to love yourself unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "You will learn to love yourself unconditionally, unconditionally, unconditionally.\n",
      "\n",
      "All of these things and more will be on my page.\n",
      "\n",
      "I hope you find this guide inspiring!\n",
      "\n",
      "If you have any questions about anything I write, I hope you can send me an email.\n",
      "\n",
      "Thank you for reading and I hope you find this guide inspiring.\n",
      "\n",
      "I hope you are getting some great things out of this program and I hope you find some great things out of this program.\n",
      "\n",
      "You can find out more on that website or you can check out my other book, The Best Life You Ever Had.\n",
      "\n",
      "You can also check out my other book\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generator(state_dict, '', unconditional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generate completion of given text\n",
    "\n",
    "Here we now get down to business: The model is more interesting if you give it context for conditional text generation. The model picks up the style and context from the input and tries to continue the 'story', complete the list, or adapt to the style. The variable `text` takes the example input that the model adapts to (defined below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:25<00:00, 19.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " It was very beautiful. And it was very beautiful. It was very beautiful to me, and I didn't want to go to school for it.\n",
      "\n",
      "And then I went to Harvard and I went to MIT and I went to college and I went to the University of Chicago. And then I went to Harvard and I went to Harvard and I went to the University of Chicago and I went to the University of Hawaii and I went to the University of California. And I went to the University of New Mexico and I went to the University of Minnesota and I went to the University of Pennsylvania. And I went to the University of Minnesota and I went to the University of North Dakota and I went to the University of North Dakota. And I went to the University of South Carolina. And I went to the University of South Dakota. And then I went to the University of South Dakota. And I went to the University of South Dakota. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And I went to the University of California. And\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Once when I was six years old I saw a magnificent picture in a book, \\\n",
    "called True Stories from Nature, about the primeval forest.'\n",
    "\n",
    "text_generator(state_dict, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Get more completion samples\n",
    "\n",
    "Modify parameter `nsamples` to set the number of generated samples.  The variable `text` holds the example input you have in Task 2. Feel free to change it when you explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/512 [00:00<01:23,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 375/512 [00:18<00:06, 20.29it/s]"
     ]
    }
   ],
   "source": [
    "text_generator(state_dict, text, nsamples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Control the length of generated completion\n",
    "\n",
    "You can also control how long text samples the method generates. The default length is 512 words, and the longest limiation is 1024 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(state_dict, text, length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Modify model parameters\n",
    "\n",
    "The model has additional parameters that you can control. The default 'temperature' parameter has value `temperature=0.7`. Play around with the model by modifying this parameter. What happens when you change the value to, e.g., 0.5 or 0.9? The variable `text` holds the example input you have in Task 4. Feel free to change it when you explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(state_dict, text, length=20, temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Play around with the model\n",
    "\n",
    "Now your task is to explore further and try out various things. Make the model\n",
    "* write a list of things to take with you to Mars (e.g., *'If I ever travel to Mars, I would take with me the following items.'*)\n",
    "* write a bed time story for children (e.g., *'It was a dark and stormy night...'*)\n",
    "* write a news story about the Corona virus pandemic\n",
    "* write you the course essay for this course\n",
    "\n",
    "Feel free to be creative and try out other things that cross your mind. Remember that running the model several times will produce different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
